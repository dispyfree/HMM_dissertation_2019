There are several approaches to solve for $\Theta$ or $\allC$, the most common being Maximum Likelihood Estimation (abbrev. MLE), Expectation-Maximization (abbrev. EM) and Markov Chain Monte Carlo Methods (abbrev. MCMC). This thesis shall explore the latter in depth. Nonetheless, we provide a brief account of the other techniques as well.

In order to outline the core ideas behind each approach, a few preliminaries are needed. These are shortly introduced. The presentation herein follows the notation in Zucchini.


\section{Forward Probabilities}
For the algorithms to follow, we need a means to compute the probability of being in a certain state at a certain time, as well as the probability of a string of observations occuring, given $\Theta$. The forward probabilities provide such a means. 

To be precise, forward probabilities provide a \textit{dynamic programming} approach to this problem; they allow us to express the solution to a bigger problem as a set of sub-problems. 


\subsection{Mathematical approach}
\label{chap_math_approach}
Suppose that we are able to compute the quantity 
\[
	\alpha_t(j) = P\left(X_1 = x_1, X_2 = x_2, \dots, X_t = x_t, C_t = j\right)
\]

For $t=0$ this is obvious, as we have $\alpha_0 = \delta$\footnote{(there are no observations available)}. 

Then we able to compute $\alpha_{t+1}(i)$ as follows:
\begin{align}
	\alpha_{t+1}(i) 
	&=  P\left(X^{(t+1)} = x^{(t+1)},  C_{t+1} = i\right) \\
	 &=  \sum_{k=1}^m P\left(X^{(t+1)} = x^{(t+1)}, C_{t+1} = i, C_t = k\right) \\ 
	 &= \sum_{k=1}^m \prob{X^{(t+1)} = x^{(t+1)}, C_t = k}{ C_{t+1} = i} P \left( C_{t+1} = i \right) \\
	 &= \sum_{k=1}^m \prob{X^{(t)} = x^{(t)}, C_t = k}{ C_{t+1} = i} \prob{X_{t+1} = x_{t+1}}{ C_{t+1} = i} P \left( C_{t+1} = i \right) \\
	 &= \sum_{k=1}^m P \left( X^{(t)} = x^{(t)}, C_t = k, C_{t+1} = i  \right) p_i(x_{t+1}) \\
	 &= \sum_{k=1}^m \prob{
	 	X^{(t)} = x^{(t)}, C_{t+1} = i}{C_t = k} P\left( C_t = k\right) p_i(x_{t+1})\\
 	&= \sum_{k=1}^m \prob{
 		X^{(t)} = x^{(t)}}{C_t = k} \prob{ C_{t+1} = i}{C_t = k} P\left( C_t = k\right) p_i(x_{t+1})\\
 	&=  \sum_{k=1}^m \alpha_t(k)  \, \tpm_{k, i} \, p_i(x_{t+1}) \\
 	\label{llrecursion}
\end{align}

Note that there are $\bigO{m}$ calculations for each time step and entry of $\alpha_t$, hence $\bigO{m^2}$ calculations for each $\alpha_t$ and $\bigO{T m^2}$ calculations for processing of the entire chain. Hence, this approach is also suitable for very long chains.
 
Also by the law of total probability, we have
\[
\sum_j \alpha_T(j) = P\left(X_1 = x_1, X_2 = x_2, \dots, X_T = x_t \right)
\] 
i.e. the probability (or density) of this string of observations occuring. 


\subsection{Computational Approach}

The computations given below solely express the approach described above in a convenient matrix notation. 
Let 
\begin{align*}
	P(x_i) := \begin{pmatrix}
		p_1(x_i) & \dots    & \dots & 0  \\
		0        & p_2(x_i) & \dots & 0  \\
		\vdots   & \vdots   & \vdots& \vdots \\ 
		0        & \dots    & \dots & p_m(x_i)
	\end{pmatrix}
\end{align*}
where $x_i$ is a realisation and $p_1, \dots, p_m$ are the states' probability functions in case the distributions are discrete or probability density functions otherwise, i.e.
\[
	p_i(x_t) := \prob{X_t = x_t}{C_t = i}
\]

Then we have:
\[
\alpha_t = \delta \, P(x_1) \, \tpm \, P(x_2) \, \dots \tpm P(x_t)
\]

Note that if the observations are not consecutive, the powers of $\tpm$ need to be adjusted accordingly:
\[
P(X_1 = x_1, X_3 = x_3, X_T = x_t) = \delta \,  P(x_1) \, \tpm^{(3-1)} \,  P(x_3) \,  \tpm^{(T-3)} \, P(x_t) \, 1^\prime
\]
where $1^\prime$ is a vertical vector with components all $1$. 



\section{Maximum Likelihood Estimation}

As we have developed a way to compute $\prob{X^T=x^t}{\Theta}$, we can use an appropriate method to maximise this expression. In fact, if we wanted to estimate $\Theta$, we could leverage Bayes' law as follows:
\begin{align}
	\underbrace{\prob{\Theta}{X^T=x^t}}_{\text{posterior distribution}} &= \frac{\prob{X^T=x^t}{\Theta} \uP{\Theta}}{\uP{X^T=x^t}} \\
		&\propto \prob{X^T=x^t}{\Theta} \uP{\Theta}  \quad \} \, \text{Bayesian estimation} 
	\label{bay_estimate}
\end{align}
$\uP{\Theta}$ is also referred to as the \textit{prior distribution}.

TODO: distinguish MLE from Bayes further

\textit{Gradient-based} methods are commonly used to (locally) estimate $\Theta$ by choosing it as to maximise the probability of the data. 


\section{Expectation Maximisation}
 
The basic idea of EM (Expectation Maximisation) is to treat the hidden states as missing data and estimate them based on an estimate of $\Theta$. Then, having \textit{complete data} at its disposal, it in turn estimates $\Theta$. 

The algorithm thus alternates between two steps: 
\begin{enumerate}
	\item \textbf{Expectation:} \\
		  Complete the data (i.e. hidden states) by using their \textit{expected values} given $\Theta$
	\item \textbf{Maximisation:} \\
		  Given complete data (i.e. also the estimated states), choose $\Theta$ as to maximise the likelihood 
\end{enumerate} 

Note that the second step is often easy to compute, as \textit{complete data} is available. The algorithm differs significantly from MCMC methods in that it 
\begin{itemize}
	\item makes point estimates of both hidden states and parameters
	\item uses the expectation for this estimate
\end{itemize}

As opposed to this, MCMC in the Bayesian context operate on \textit{distributions} of parameters. The target values in EM are hence distributions in MCMC and the expectation in EM is replaced by sampling from a posterior distribution in MCMC. 


\section{Markov Chain Monte Carlo Methods}

	In a Bayesian understanding, model parameters are not just values, but are thought of as distributions themselves.
	
	Unfortunately, the complexity of the problems at hand does often not allow for an analytical solution to the parameters' posterior distribution. 
	Still, we would like to gain an understanding of the distributions' shape. Aside from the general shape of the posterior distributions, point estimates like mean, variance, Kurtosis etc. are also of interest. 
	
	A canonical solution to this problem is to sample $\Theta_1, \Theta_2, \dots$ independently from the posterior distribution. By the central limit theorem (CLT) and the delta method, we can then approximate all quantities of interest to arbitrary accuracy as the length of the sample approaches infinity. 
	
	In reality, the algorithms we use for sampling (except for Rejection-Sampling) generally do not produce mutually independent samples; hence $\left(\Theta_1, \Theta_2, \dots \right)$ is also referred to as a \textit{chain}. Among others, the level of (in)dependence of the samples is a quality measure of the chain itself. For details on how we ascertain the sampler's quality, please refer to the respective chapter of measures.   
	
	
	As aforementioned, the EM algorithm operates on maxima and hence can be thought of a specific case of the Bayesian interpretation - namely, operating only on the point estimates instead of the distributions themselves. These point estimates are often computed by analytically maximising likelihood; hence no sampling is required. On the other hand, as point estimates, they provide less information than the posterior distributions one obtains in a Bayesian framework. 
	
	

	\subsection{Metropolis-Hastings Sampler}
	\label{chap_mh_sampler}
	The Metropolis-Hastings Sampler (abbrev. MH) can be thought of as a \textit{stochastic gradient descent}. 
	
	Starting from a given initial value $\Theta_1$,  MH sampler produces a Markov Chain of $\left(\Theta_1, \Theta_2, \dots \right)$ by repeatedly choosing a new theta in the direct vicinity of the previous one. 
	
	A new estimate of $\Theta$ is obtained by modifying the current sample slightly (usually with a step sampled from a normal distribution), estimating the new probability using \ref{bay_estimate} and accepting the sample based on this new probability. 
	
	Hence, a Metropolis Hastings Sampler can be defined by providing
	\begin{itemize}
		\item $\tip{\Theta} := \prob{\Theta}{X^T = x^T}$\\
			  The posterior probability of the new sample 
		\item $Q(\Theta, \tp)$ \\
			  The probability of considering $\tp$ as the new candidate when $\Theta$ is the current candidate. \\
			  That is, the new candidate is sampled from $Q(\Theta, \cdot)$. 
	\end{itemize}
	
	After drawing a new sample $\tp$ starting from $\Theta$, $\tp$ is accepted depending on the following:
	\begin{enumerate}
		\item Let 
		\begin{align}
			\alpha := min \Bigg\{ 1,  \frac{\tip{\tp}}{\tip{\Theta}} \, \frac{Q(\tp, \Theta)}{Q(\Theta, \tp)}			
			\Bigg\}
			\label{alphaSamplingStep}
		\end{align}
		\item Draw $u \sim U[0, 1]$
		\item If $u \leq \alpha$, accept $\tp$ (i.e. set $\Theta_{t+1} = \tp$), otherwise reject it (i.e. $\Theta_{t+1} = \Theta_t$). 
	\end{enumerate} 
	If the probability of acceptance is low, then the chain often rests at the same position and hence exhibits a high autocorrelation - the chain is then also called ''sticky''. This is obviously detrimental for statistical independence and hence reduces the information inherent to the generated chain. 
	The measure of \textit{effective sample size} tries to capture the loss of information due to this effect and shall be elaborated upon later on.

	We will be using a symmetric $Q$ for this thesis; note that in this case,  $\alpha$ simplifies to 
	\[
			\alpha := min \Bigg\{ 1,  \frac{\tip{\tp}}{\tip{\Theta}}			
		\Bigg\}
	\]
	
	
	Note that convergence of the chain produced by this sampler is by no means obvious; \cite{mcnotes} may be consulted for details. 
	
	
	
	\subsection{Gibb's Sampler}
	The Gibb's sampler samples by repeatedly sampling one component while fixing all other components. When fixing all other components, a the \textit{joint distribution} becomes a \textit{marginal distribution}, which is often much easier to sample from. In other words:
	
	To illustrate the Gibb's sampler, let $Y$ be an m-dimensional vector parameterising a given model completely. Let $Y_{-i}$ denote all the parameters \textit{except for} the ith parameter.
	We again consider a chain $Y^{(1)}, Y^{(2)}, \dots$ of parameters; $Y_i^{(t)}$ then denotes the ith component of the parameterisation at time step $t$. 
	
	The Gibb's sampler works as follows in each step:
	\begin{itemize}
		\item Draw index $i \in \{1, \dots, m\}$ uniformy
		\item Draw $\tilde{Y}^{(t+1)}_i \sim \prob{Y^{(t)}_i}{Y^{(t)}_{-i}}$
		\item let $Y^{(t+1)} := \left(
				Y^{(t)}_1, Y^{(t)}_2, \dots, Y^{(t)}_{i-1}, 
				\tilde{Y}^{(t+1)}_i, 
				Y^{(t)}_{i+1}, \dots, Y^{(t)}_m
			\right)$
	\end{itemize}
	
	The variant presented herein is called the \textit{Random Scan Gibb's Sampling} because of the uniform choice of $i$. When $i$ is chosen successively (for instance to ensure that all parameters are regularly updated), this is referred to as the \textit{Systematic Scan Gibb's Sampler}.
	
	The Gibb's sampler is in fact a special case of the Metropolis-Hastings sampler. Note that 
	\[
		Q(Y^{(t)}, Y^{(t+1)}) = \frac{1}{m} \, \prob{Y^{(t+1)}_i}{Y^{(t)}_{-i}}
	\]
	where $i$ is the altered component. Then, in equation $\ref{alphaSamplingStep}$, we have:
	\begin{align*}
		\frac{P(  Y^{(t+1)}_i  )}{P( Y^{(t)}_i  )} \, 
		\frac{ Q( Y^{(t+1)}, Y^{(t)})}{
			Q( Y^{(t)}, Y^{(t+1)})} &=
			\frac{\uP{Y^{(t+1)}}}{ \uP{ Y^{(t)}  }}
			\frac{
				 \gibbsC{t}{i}{t+1}{-i}	
				}{
				\gibbsC{t+1}{i}{t}{-i}	
		} \\
	 &= 
	 \frac{
	 	 \gibbsC{t+1}{i}{t}{-i}
		  }{
		 \gibbsC{t}{i}{t}{-i}
 	 }
  		\,
  	\frac{
  		\uP{Y^{(t)}_{-i}}
  	}{
  		\uP{Y^{(t)}_{-i}}
	}\,
	 \frac{
	 	\gibbsC{t}{i}{t+1}{-i}	
	 }{
	 \gibbsC{t+1}{i}{t}{-i}		
	 } \\
    &= 1
	\end{align*}
	
	Hence we have $\alpha = min \{1, 1\} = 1$ and all samples drawn are accepted. 
		
	
	
	
	
	